{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the economic recovery whole user journey subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach does not rely on the existing knowledge graph. It uses a functional graph based on page hit session data, to find a list of pages related to the economic recovery whole user journey (WUJ).\n",
    "\n",
    "ASSUMPTIONS: \n",
    "- A set of pre-defined pages have been removed from the economic recovery WUJ subgraph\n",
    "- Any pages with less than 10 session hits are removed from the economic recovery WUJ subgraph\n",
    "- Any pages with a shortest page path length equal to or greater than 3 from both `seed0` pages are removed from the economic recovery WUJ subgraph\n",
    "- Any pages where accumulated edge weight is equal to or lower than 20 are removed from the economic recovery WUJ subgraph\n",
    "- Any browse pages not related to the economic recovery WUJ are manually removed from the economic recovery WUJ subgraph\n",
    "\n",
    "OUTPUT: \n",
    "- A csv containing a list of pages related to the economic recovery WUJ. Sorted in ascending order in relation to a composite metric made up of: shortest page path from both `seed0` pages, and degree centrality.\n",
    "  - Additional columns to filter on are included: `document type`, `document supertype`, `number of sessions that visit this page`, `number of sessions where this page is an entrance hit`, `number of sessions where this page is an exit hit`, `distance from /browse/working/finding-job`, `distance from /topic/further-education-skills/apprenticeships`, `the number of pages a user moves to/from between this page and another page in the list`\n",
    "\n",
    "REQUIREMENTS: \n",
    "- Run `step_one_identify_seed_pages.ipynb` to define `seed0` and `seed1` pages\n",
    "- Run `step_two_extract_page_hits.sql` to extract page hits for sessions that visit at least one `seed0` or `seed1` page\n",
    "- Run `step_three_extract_nodes_and_edges.sql` to extract nodes and edges \n",
    "- Run `step_four_create_networkx_graph.ipynb` to create NetworkX graph of the economic recovery whole user journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import gspread \n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for coercing knowledge graph into NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubgraph(q, parameters=None):\n",
    "\n",
    "    '''\n",
    "    Given a Cypher query q, this function queries the knowledge graph,\n",
    "    returns the nodes and edges from this query, and uses them to construct\n",
    "    a networkx graph.\n",
    "\n",
    "    E.g. getSubgraph(r'MATCH (u:Cid)-[r:HYPERLINKS_TO]->(v:Cid) RETURN *')\n",
    "         returns the structural graph.\n",
    "\n",
    "    Optionally, can add in parameters (dictionary), allowing Python variables\n",
    "    to be integrated into the Cypher query q.\n",
    "\n",
    "    E.g.\n",
    "        parameters = {}\n",
    "        parameters['pages'] = ['a','list','of','stuff']\n",
    "        q7 = f\"\"\"\n",
    "        MATCH (u:Cid)-[r]-(v:Cid)\n",
    "        WHERE u.name IN $pages AND v.name in $pages\n",
    "        RETURN *\n",
    "        \"\"\"\n",
    "\n",
    "        g7 = getSubgraph(q7, parameters)\n",
    "    '''\n",
    "\n",
    "    # get credentials\n",
    "    # add to .secrets: export KG_PWD=\"<PASSWORD>\"\n",
    "    KG_PWD = os.getenv(\"KG_PWD\")\n",
    "\n",
    "    # create connection to knowledge graph\n",
    "    driver = GraphDatabase.driver(\n",
    "        \"bolt+s://knowledge-graph.integration.govuk.digital:7687\",\n",
    "        auth=(\"neo4j\", KG_PWD),\n",
    "    )\n",
    "\n",
    "    # run query on knowledge graph\n",
    "    results = driver.session().run(q, parameters)\n",
    "\n",
    "    # create networkx graph object\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # add nodes into networkx graph object\n",
    "    nodes = list(results.graph()._nodes.values())\n",
    "    print(\"Adding nodes\\n\")\n",
    "    for node in tqdm(nodes):\n",
    "        G.add_node(node.id, labels=node._labels, properties=node._properties)\n",
    "\n",
    "    # add edges into networkx graph object\n",
    "    rels = list(results.graph()._relationships.values())\n",
    "    print(\"Adding edges\\n\")\n",
    "    for rel in tqdm(rels):\n",
    "        G.add_edge(\n",
    "            rel.start_node.id,\n",
    "            rel.end_node.id,\n",
    "            key=rel.id,\n",
    "            type=rel.type,\n",
    "            properties=rel._properties,\n",
    "        )\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def showGraph(g):\n",
    "    \"\"\"\n",
    "    Given a networkx graph g, this function visualises the graph.\n",
    "    Do not use for a large g.\n",
    "    \"\"\"\n",
    "    print(nx.info(g))\n",
    "    nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoOfTruePages(g):\n",
    "    \"\"\"\n",
    "    Calculate a proxy recall metric for the list of pages identified in a\n",
    "    subgraph (when compared to the ground truth for the economic recovery pages). \n",
    "    The output is the number of pages in the subgraph list that are present in \n",
    "    the ground truth list.  \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert nodeIds to page path slug for the subgraph list\n",
    "    subgraph_list = [node[0] for node in g.nodes(data=True)]\n",
    "\n",
    "    # set up the ground truth list\n",
    "    true_list = list(economic_pages)\n",
    "\n",
    "    # how many pages are in the subgraph list that are also in the ground truth list\n",
    "    return (len(true_list)) - (len([node for node in true_list if node not in subgraph_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPagesNotInSubGraph(g):\n",
    "    \"\"\"\n",
    "    Return a set of pages which are in the economic recovery pre-defined list\n",
    "    (41 in total), but are not in the filtered subgraph list. \n",
    "    \"\"\"\n",
    "    \n",
    "    # list of pages manually defined in the economic recovery whole user journey \n",
    "    true_list = list(economic_pages)\n",
    "    \n",
    "    # list of pages in the filtered subgraph list\n",
    "    subgraph_list = [node[0] for node in g.nodes(data=True)]\n",
    "    \n",
    "    # the list of pages in the manually defined list not in the subgraph list\n",
    "    return [node for node in true_list if node not in subgraph_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defined economic recovery subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pages (nodes) in the manually defined list for economic recovery (41 pages in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Connect to service account\n",
    "scope = ['https://spreadsheets.google.com/feeds'] \n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_APPLICATION_CREDENTIALS, scope) \n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "# Import the data from google sheets\n",
    "spreadsheet_key = '1lLsgQRsl4bXmbyiwrbMNQOR_Zu-FkbqYRUmOOhTYc70' \n",
    "book = gc.open_by_key(spreadsheet_key) \n",
    "worksheet = book.worksheet('Top pages') \n",
    "table = worksheet.get_all_values()\n",
    "\n",
    "# Convert table data into a dataframe then set \n",
    "df = pd.DataFrame(table[1:], columns=table[0])\n",
    "economic_pages = set(list(filter(None, df['pagePathv2'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the economic recovery functional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 10631 nodes and 79985 edges\n",
    "- The page `/guidance/international-trade-products-and-schemes` which is in the manually defined graph, is **not** in this subgraph. This seems to be an old page (updated 2013) related to exporting agricultural products to the EU\n",
    "- The distribution of session hit data demonstrates that only 25% of the page paths have session hits over 8. The maximum number of session hit for a page path is 71667: \n",
    "    - Min: 1\n",
    "    - Max: 71667\n",
    "    - 25th percentile: 1\n",
    "    - 50th percentile: 2\n",
    "    - 75th percentile: 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the NetworkX graph object\n",
    "g = nx.read_gpickle(\"../../data/processed/functional_session_hit_directed_graph_er.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DiGraph with 10631 nodes and 79985 edges'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of nodes, number of edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the 'true' economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "true_list = list(economic_pages)\n",
    "subgraph_list = [node[0] for node in g.nodes(data=True)]\n",
    "[node for node in true_list if node not in subgraph_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of nodes\n",
    "nodes_list = list(g.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of session hit data \n",
    "x = [y['sessionHitsAll'] for x, y in nodes_list]\n",
    "\n",
    "plt.hist(x); # , bins = 10\n",
    "plt.xlabel('Session hits')\n",
    "plt.ylabel('Count')\n",
    "plt.title('No. of page paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot to identify outliers\n",
    "plt.boxplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptives, and remove outliers in boxplot\n",
    "plt.boxplot(x, showfliers=False)\n",
    "pd.DataFrame(x).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distribution of session hit data up to 100\n",
    "x = [y['sessionHitsAll'] for x, y in nodes_list]\n",
    "x = sorted(x, reverse=True)\n",
    "x = x[2:]\n",
    "plt.hist(x,bins=10,range=(0,100))\n",
    "plt.xlabel('Session hits')\n",
    "plt.ylabel('Count')\n",
    "plt.title('No. of session hits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the precision of the subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall of the subgraph (10631 nodes) is acceptable. However, we need to improve the precision (i.e. reduce the number of nodes) so that a human can manually trawl through the list of pages, and not be overwhelmed. We assume the pages removed are irrelevant.\n",
    "\n",
    "- Remove **irrelevant pages**. There are a number of pages that have high session hit data and are not relevant to the WUJ. These should be predefined and removed. While some would be relevant to remove in all graphs (e.g. `/search/all`), other pages are more unique, but patterns emerge. For example, pages related to `pensions` and `criminal convinctions`. \n",
    "\n",
    "- Remove **session hits equal to or lower than 10**.  \n",
    "\n",
    "- Remove pages where **shortest page path length from is equal to or greater than 3, from both seed0 pages**\n",
    "\n",
    "- Remove pages where the **accumulated edge weight for one page is equal to or greater than 20**\n",
    "\n",
    "This leaves us with a subgraph of: \n",
    "- 1338 nodes\n",
    "- 35/41 nodes in the subgraph, which are also in the manually defined list of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrelevant pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order pages in descending order related to session hits\n",
    "sorted(g.nodes(data=True), key=lambda x: x[1]['sessionHitsAll'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nodes \n",
    "g.remove_nodes_from(['/prove-right-to-work', '/', '/request-copy-criminal-record', '/browse/working/state-pension', \n",
    "                     '/search/all','/search', '/brexit', '/coronavirus', '/report-covid19-result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pages with session hits equal or lower than 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide a cut-off to remove pages with sessions hits lower than X. 10 seems to contain a good number of pre-defined pages (38/41), and reduces the number of nodes in the graph drastically (10631 > 2246).  \n",
    "- Equal or lower than 10: 2246 nodes, 38/41, [`/find-driving-instructor-training`, `/guidance/international-trade-products-and-schemes`, `/how-to-claim-universal-credit`]\n",
    "- Equal or lower than 20: 1150 nodes, 38/41, [`/guidance/recovery-loan-scheme`, `/find-driving-instructor-training`, `/guidance/international-trade-products-and-schemes`, `/how-to-claim-universal-credit`]\n",
    "- Equal or lower than 30: 1355 nodes, 34/41, [`/guidance/recovery-loan-scheme`, `/find-driving-instructor-training`,\n",
    " `/education`, `/guidance/international-trade-products-and-schemes`, `/agricultural-skills-and-training`, `/government/collections/financial-support-for-businesses-during-coronavirus-covid-19`, `/how-to-claim-universal-credit`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table for session hits\n",
    "x = sorted([y['sessionHitsAll'] for x, y in nodes_list], reverse=True)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal to or lower than 10 sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages with less than 10 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHitsAll'] <= 10]\n",
    "\n",
    "# remove nodes with less than 10 session hits \n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# how many nodes and edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the start a business pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal to or lower than 20 sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages with less than 20 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHitsAll'] <= 20]\n",
    "\n",
    "# remove nodes with less than 20 session hits \n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# how many nodes and edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the economic recovery pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal to or lower than 30 sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages with less than 30 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHitsAll'] <= 30]\n",
    "\n",
    "# remove nodes with less than 30 session hits \n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# how many nodes and edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the start a business pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pages with max shortest page paths from `seed0` pages >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the shortest page paths from seed0 pages to all other pages. \n",
    "\n",
    "Frequency distribution of shortest page path lengths: \n",
    "- `seed0.1`; /browse/working/finding-job; {4: 2, 3: 137, 2: 1272, 1: 138, 0: 1}\n",
    "- `seed0.2`: /topic/further-education-skills/apprenticeships; {4: 5, 3: 392, 2: 1047, 1: 105, 0: 1}\n",
    "\n",
    "\n",
    "If the shortest page path is equal to or greater than 3, remove from the nodes from the subgraph:\n",
    "- 1521 nodes\n",
    "- 37/41 \n",
    "- Pages in the manually defined list not in the subgraph: [`/guidance/international-trade-products-and-schemes`,\n",
    " `/guidance/recovery-loan-scheme`, `/find-driving-instructor-training`, `/how-to-claim-universal-credit`, `/government/collections/financial-support-for-businesses-during-coronavirus-covid-19`, `/agricultural-skills-and-training`, `/education`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort nodes by closeness to seed 0 pages\n",
    "shortest_paths_seed0_1 = nx.shortest_path_length(g, source='/browse/working/finding-job')\n",
    "shortest_paths_seed0_2 = nx.shortest_path_length(g, source='/topic/further-education-skills/apprenticeships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort nodes in descending order\n",
    "finding_job = {k: v for k, v in sorted(shortest_paths_seed0_1.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort nodes in descending order\n",
    "apprentice = {k: v for k, v in sorted(shortest_paths_seed0_2.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table for `seed0.1`\n",
    "x = sorted([y for x, y in finding_job.items()], reverse=True)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table for `seed0.2`\n",
    "x = sorted([y for x, y in apprentice.items()], reverse=True)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of pages that have a shortest page path length of >= 3\n",
    "finding_job_filtered = [node for node, length in finding_job.items() if length >= 3]\n",
    "apprentice_filtered = [node for node, length in apprentice.items() if length >= 3]\n",
    "\n",
    "# extract list of page where they have a shortest page path length on >= 3 from both `seed0` pages\n",
    "remove=[node for node in finding_job_filtered if node not in apprentice_filtered]\n",
    "\n",
    "# remove list of pages from g\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the economic recovery pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: remove pages with the shortest page paths summed (`seed0.1` + `seed0.2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could also look at the sum of the shortest page paths. However, the caveat here is that if the shortest page path length is `1` from `seed0.1` to `page A`, but `6` from `seed0.2` to `page A`, then the sum would be `7`. However, `page A` may be very relevant to the WUJ, as defined by it's shortest page path length from `seed0.1`.  \n",
    "\n",
    "Therefore, this method has not been chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum all pages shortest page paths of both `seed0` nodes to get an overall shortest page path proxy metric\n",
    "combined_paths = {k: finding_job[k] + apprentice[k] for k in set(finding_job) & set(apprentice)}\n",
    "combined_paths = {k: v for k, v in sorted(combined_paths.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nodes above or equal to 6 paths away  \n",
    "remove = [node for node, length in combined_paths.items() if length >= 6]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pages where edge weight is <= 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge weight increases by 1 each time a user session visits page A to page B. Therefore, if edge weight is small, we assume the two pages are likely to not be associated with one another. This is because we assume that user's will visit similar pages in the same WUJ during the same session. \n",
    "\n",
    "Edge weight is equal to or less than 10:\n",
    "- 1473 nodes\n",
    "- 37/41 \n",
    "- Pages not in the subgraph list: [`/guidance/recovery-loan-scheme`, \n",
    "`/find-driving-instructor-training`, \n",
    "`/guidance/international-trade-products-and-schemes`,\n",
    "`/how-to-claim-universal-credit`]\n",
    "\n",
    "Edge weight is equal to or less than 20:\n",
    "- 1253 nodes\n",
    "- 34/41 \n",
    "- Pages not in the subgraph list: [`/guidance/international-trade-products-and-schemes`,\n",
    " `/guidance/recovery-loan-scheme`,\n",
    " `/find-driving-instructor-training`,\n",
    " `/how-to-claim-universal-credit`,\n",
    " `/government/collections/financial-support-for-businesses-during-coronavirus-covid-19`,\n",
    " `/agricultural-skills-and-training`,\n",
    " `/guidance/claim-back-statutory-sick-pay-paid-to-employees-due-to-coronavirus-covid-19`]\n",
    "\n",
    "Edge weight is equal to or less than 30:\n",
    "- 1042 nodes\n",
    "- 32/41 \n",
    "- Pages not in the subgraph list: [`/guidance/international-trade-products-and-schemes`,\n",
    " `/guidance/recovery-loan-scheme`,\n",
    " `/find-driving-instructor-training`,\n",
    " `/how-to-claim-universal-credit`,\n",
    " `/government/collections/financial-support-for-businesses-during-coronavirus-covid-19`,\n",
    " `/agricultural-skills-and-training`,\n",
    " `/guidance/claim-back-statutory-sick-pay-paid-to-employees-due-to-coronavirus-covid-19`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight (user movement) for each pair of nodes in the graph  \n",
    "user_movement_weights = []\n",
    "for node1, node2, edgeWeight in g.edges(data=True):\n",
    "    case = {'node1': node1, 'node2': node2, 'edgeWeight':edgeWeight['edgeWeight']}\n",
    "    user_movement_weights.append(case)\n",
    "\n",
    "# sorted(user_movement_weights, key=itemgetter('edgeWeight'), reverse=True)\n",
    "\n",
    "# sum the weight for each node \n",
    "user_movements_sum = defaultdict(float)\n",
    "\n",
    "for info in user_movement_weights:\n",
    "    user_movements_sum[info['node1']] += info['edgeWeight']\n",
    "\n",
    "user_movements_sum = [{'node1': node1, 'edgeWeight': user_movements_sum[node1]} \n",
    "                     for node1 in user_movements_sum]\n",
    "\n",
    "#sorted(user_movements_sum, key=lambda x: x['edgeWeight'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pages where edge weight is less than or equal to 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything with edge weight less than 10 \n",
    "remove = [node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 10]\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pages where edge weight is less than or equal to 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything with edge weight less than 20 \n",
    "remove = [node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 20]\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pages where edge weight is less than or equal to 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything with edge weight less than 30 \n",
    "remove = [node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 30]\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the subgraph list of pages that are related to the economic recovery whole user journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above exploration, run this code to get a final subgraph related to the economic recovery whole user journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the NetworkX graph object\n",
    "g = nx.read_gpickle(\"../../data/processed/functional_session_hit_directed_graph_er.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant nodes \n",
    "g.remove_nodes_from(['/prove-right-to-work', '/', '/request-copy-criminal-record', '/browse/working/state-pension', \n",
    "                     '/search/all','/search', '/brexit', '/coronavirus', '/report-covid19-result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pages with equal or less than 10 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHitsAll'] <= 10]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pages where they have a shortest page path length on >= 3 from both `seed0` pages\n",
    "shortest_paths_seed0_1 = nx.shortest_path_length(g, source='/browse/working/finding-job')\n",
    "shortest_paths_seed0_2 = nx.shortest_path_length(g, source='/topic/further-education-skills/apprenticeships')\n",
    "\n",
    "finding_job = {k: v for k, v in sorted(shortest_paths_seed0_1.items(), key=lambda item: item[1], reverse=True)}\n",
    "apprentice = {k: v for k, v in sorted(shortest_paths_seed0_2.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "finding_job_filtered = [node for node, length in finding_job.items() if length >= 3]\n",
    "apprentice_filtered = [node for node, length in apprentice.items() if length >= 3]\n",
    "\n",
    "remove = [node for node in finding_job_filtered if node not in apprentice_filtered]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pages where accumulated edge weight is equal to or lower than 20\n",
    "user_movement_weights = []\n",
    "for node1, node2, edgeWeight in g.edges(data=True):\n",
    "    case = {'node1': node1, 'node2': node2, 'edgeWeight':edgeWeight['edgeWeight']}\n",
    "    user_movement_weights.append(case)\n",
    "\n",
    "user_movements_sum = defaultdict(float)\n",
    "\n",
    "for info in user_movement_weights:\n",
    "    user_movements_sum[info['node1']] += info['edgeWeight']\n",
    "\n",
    "user_movements_sum = [{'node1': node1, 'edgeWeight': user_movements_sum[node1]} \n",
    "                     for node1 in user_movements_sum]\n",
    "\n",
    "remove=[node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 20]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove certain browse pages. Browse pages seem to score highly (when ranking, below), \n",
    "# regardless of whether they are relevant to the WUJ or not\n",
    "browse_pages_remove = ('/browse/employing-people', '/browse/births-deaths-marriages', \n",
    "                       '/browse/citizenship', '/browse/driving', '/browse/education', \n",
    "                       '/browse/business', '/browse/childcare-parenting', '/browse/justice', \n",
    "                       '/browse/abroad', '/browse/tax', '/browse/visas-immigration', \n",
    "                       '/browse/disabilities', '/browse/environment-countryside', \n",
    "                       '/browse/housing-local-services', '/browse/employing-people', \n",
    "                       '/browse/births-deaths-marriages', '/browse/citizenship', \n",
    "                       '/browse/driving', '/browse/business', '/browse/abroad', \n",
    "                       '/browse/environment-countryside', '/browse/housing-local-services',\n",
    "                       '/browse/benefits')\n",
    "\n",
    "g.remove_nodes_from(browse_pages_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DiGraph with 1338 nodes and 31143 edges'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/find-driving-instructor-training',\n",
       " '/agricultural-skills-and-training',\n",
       " '/how-to-claim-universal-credit',\n",
       " '/guidance/recovery-loan-scheme',\n",
       " '/browse/education',\n",
       " '/guidance/international-trade-products-and-schemes']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final subgraph list related to the economic recovery whole user journey\n",
    "economic_recovery_subgraph = list(g.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save subgraph to file\n",
    "nx.write_gpickle(g, '../../data/processed/functional_session_hit_directed_graph_er_final.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore communities of the subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at adjacent neighbours, to identify communities within the WUJ. While there seems to be relevant pages within a community (e.g. `/find-a-job` and `/contact-jobcentre-plus`), other pages seem to be irrelevant (e.g. `/find-a-job` and `/check-mot-status`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of all nodes and it's edges\n",
    "dict_of_nodes_and_edges = dict()\n",
    "for node in g.nodes():\n",
    "    dict_of_nodes_and_edges[node] = list(nx.neighbors(g, node))\n",
    "\n",
    "    # sort dict based on the length of its' values (i.e. top result is the largest number of neighbours)\n",
    "sorted(dict_of_nodes_and_edges.items(), key=lambda x: len(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking the subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a subgraph of 1384 nodes. How do we rank these pages so that the most 'relevant' pages to a WUJ are at the top, while the most 'irrelevant' pages to a WUJ are at the bottom? \n",
    "\n",
    "Explored:\n",
    "- session hits \n",
    "- shortest page path lengths \n",
    "- sum of the shortest page path lengths\n",
    "- between centrality\n",
    "- degree centrality \n",
    "- closeness centrality \n",
    "\n",
    "An average (non-weighted) rank for each page path was created using:\n",
    "- shortest page path length from seed0.1: `shortest_seed0_1`\n",
    "- shortest page path length from seed0.2: `shortest_seed0_2`\n",
    "- degree centrality: `g_degree`\n",
    "\n",
    "These four metrics were chosen as the assumption is that nodes close to seed0 nodes, are more likely to be associated with the same topic, and therefore the same WUJ. In addition, nodes which are more highly connected (e.g. more outgoing/incoming relationships) should be included in the WUJ, as it is expected the majority of the pages in the subgraph are mostly relevant.   \n",
    "\n",
    "The others metrics were not chosen, because: \n",
    "- session hits: pages with low session hits may be relevant to a WUJ\n",
    "- sum of the shortest page path lengths: pages may be closer to one seed0 node, but far away from the other seed0 node\n",
    "- between centrality: pages relevant to a WUJ may not neccessarily have many paths that pass through. It is likely that more 'popular' pages will (i.e. ones with higher session hits)\n",
    "\n",
    "\n",
    "Caveat: browse pages are likely to be ranked highly, as these are common pages which may be highly connected to other nodes. Therefore, irrelevant browse pages are removed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by session hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = list(sorted(economic_recovery_subgraph, key=lambda x: x[1]['sessionHitsAll'], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by shortest page path lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the paths of the shortest page path\n",
    "shortest_paths_seed0 = nx.shortest_path_length(g, source='/browse/working/finding-job')\n",
    "shortest_paths_seed1 = nx.shortest_path_length(g, source='/topic/further-education-skills/apprenticeships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeed0.1: shortest_paths_seed0.1: '/browse/working/finding-job'\n",
    "shortest_seed0_1 = {k: v for k, v in sorted(shortest_paths_seed0.items(), key=lambda item: item[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed0.2: shortest_paths_seed0.2: '/topic/further-education-skills/apprenticeships'\n",
    "shortest_seed0_2 = {k: v for k, v in sorted(shortest_paths_seed1.items(), key=lambda item: item[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by the sum of the shortest page path lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paths = {k: finding_job[k] + apprentice[k] for k in set(finding_job) & set(apprentice)}\n",
    "combined_paths = {k: v for k, v in sorted(combined_paths.items(), key=lambda item: item[1], reverse=True)}\n",
    "combined_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by centrality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between centrality: the number of shortest paths that pass through the node - a 'bridge' between nodes\n",
    "# higher values indicate higher centrality\n",
    "g_di = nx.DiGraph(g)\n",
    "g_di_between = nx.betweenness_centrality(g_di)\n",
    "g_between = dict(sorted(g_di_between.items(), key=itemgetter(1),reverse=True))\n",
    "g_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree centrality: counts the number of incoming and outgoing relationships from a node - 'most connected'\n",
    "# the higher the degree, the more central the node is\n",
    "g_degree = nx.degree_centrality(g)\n",
    "g_degree = dict(sorted(g_degree.items(), key=itemgetter(1),reverse=True))\n",
    "g_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closeness centrality: average length of the shortest path between the node and all other nodes in the graph\n",
    "# higher values of closeness indicate higher centrality\n",
    "g_closeness = nx.closeness_centrality(g)\n",
    "g_closeness = dict(sorted(g_closeness.items(), key=itemgetter(1),reverse=False))\n",
    "g_closeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall rank from multiple ranked items\n",
    "\n",
    "Choose a final ranking method. In this example, `shortest_seed0_1`, `shortest_seed0_2`, `g_degree` have been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists in order\n",
    "sessions_list = [(index, element[0]) for index, element in enumerate(sessions)]\n",
    "shortest_seed0_1_list = [(index, element) for index, element in enumerate(shortest_seed0_1)]\n",
    "shortest_seed0_2_list = [(index, element) for index, element in enumerate(shortest_seed0_2)]\n",
    "combined_paths_list = [(index, element) for index, element in enumerate(combined_paths)]\n",
    "g_between_list = [(index, element) for index, element in enumerate(g_between)]\n",
    "g_degree_list = [(index, element) for index, element in enumerate(g_degree)]\n",
    "g_closeness_list = [(index, element) for index, element in enumerate(g_closeness)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average of rank for each page path (non-weighted)\n",
    "ranked_items = pd.DataFrame.from_records(shortest_seed0_1_list+shortest_seed0_2_list+g_degree_list).groupby(1).mean().round().reset_index()\n",
    "ranked_items = ranked_items.rename(columns={1: 'page', 0: 'rank'})\n",
    "ranked_items = ranked_items.sort_values(by=['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download as csv file\n",
    "ranked_items.to_csv('../../data/processed/ranked_items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add document type and session hit data to final csv file\n",
    "\n",
    "Document supertypes as per: https://docs.publishing.service.gov.uk/document-types/content_purpose_supergroup.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract: documentType, sessionHitsAll, entranceHit, exitHit, entranceAndExitHit\n",
    "df_page = pd.DataFrame([i[0] for i in economic_recovery_subgraph], columns=['page'])\n",
    "df_info = pd.DataFrame([i[1] for i in economic_recovery_subgraph])\n",
    "df_all = df_page.join(df_info)\n",
    "\n",
    "# define a set for news and communication doc types\n",
    "news_and_comms_doctypes = {'medical_safety_alert', 'drug_safety_update', 'news_article', \n",
    "                           'news_story', 'press_release', 'world_location_news_article', \n",
    "                           'world_news_story', 'fatality_notice', 'fatality_notice', \n",
    "                           'tax_tribunal_decision', 'utaac_decision', 'asylum_support_decision', \n",
    "                           'employment_appeal_tribunal_decision', 'employment_tribunal_decision', \n",
    "                           'employment_tribunal_decision', 'service_standard_report', 'cma_case', \n",
    "                           'decision', 'oral_statement', 'written_statement', 'authored_article', \n",
    "                           'correspondence', 'speech', 'government_response', 'case_study' \n",
    "}\n",
    "\n",
    "# define a set for service doc types\n",
    "service_doctypes = {'completed_transaction', 'local_transaction', 'form', 'calculator',\n",
    "                    'smart_answer', 'simple_smart_answer', 'place', 'licence', 'step_by_step_nav', \n",
    "                    'transaction', 'answer', 'guide'\n",
    "}\n",
    "\n",
    "# define a set for guidance and regulation doc types\n",
    "guidance_and_reg_doctypes = {'regulation', 'detailed_guide', 'manual', 'manual_section',\n",
    "                             'guidance', 'map', 'calendar', 'statutory_guidance', 'notice',\n",
    "                             'international_treaty', 'travel_advice', 'promotional', \n",
    "                             'international_development_fund', 'countryside_stewardship_grant',\n",
    "                             'esi_fund', 'business_finance_support_scheme', 'statutory_instrument',\n",
    "                             'hmrc_manual', 'standard'\n",
    "}\n",
    "\n",
    "# define a set for policy and engagement doc types\n",
    "policy_and_engage_doctypes = {'impact_assessment', 'policy_paper', 'open_consultation',\n",
    "                              'policy_paper', 'closed_consultation', 'consultation_outcome',\n",
    "                              'policy_and_engagement'  \n",
    "}\n",
    "\n",
    "# define a set for research and statistics doc types\n",
    "research_and_stats_doctypes = {'dfid_research_output', 'independent_report', 'research', \n",
    "                               'statistics', 'national_statistics', 'statistics_announcement',\n",
    "                               'national_statistics_announcement', 'official_statistics_announcement',\n",
    "                               'statistical_data_set', 'official_statistics'\n",
    "}\n",
    "\n",
    "# define a set for transparency doc types\n",
    "transparency_doctypes = {'transparency', 'corporate_report', 'foi_release', 'aaib_report',\n",
    "                         'raib_report', 'maib_report'\n",
    "}\n",
    "\n",
    "# loop through document types and create document supertype column \n",
    "document_type_dict = dict.fromkeys(list(set(df_info['documentType'])))\n",
    "\n",
    "for docType, docSupertype in document_type_dict.items():\n",
    "    if docType in news_and_comms_doctypes: \n",
    "        document_type_dict[docType] = 'news and communication'\n",
    "    \n",
    "    elif docType in service_doctypes:\n",
    "        document_type_dict[docType] = 'services'\n",
    "    \n",
    "    elif docType in guidance_and_reg_doctypes:\n",
    "        document_type_dict[docType] = 'guidance and regulation'\n",
    " \n",
    "    elif docType in policy_and_engage_doctypes:\n",
    "        document_type_dict[docType] = 'policy and engagement'\n",
    "    \n",
    "    elif docType in research_and_stats_doctypes:\n",
    "        document_type_dict[docType] = 'research and statistics'\n",
    "    \n",
    "    elif docType in transparency_doctypes:\n",
    "        document_type_dict[docType] = 'transparency'\n",
    "    \n",
    "    else: \n",
    "        document_type_dict[docType] = 'other' \n",
    "\n",
    "df_docSuper = pd.DataFrame(document_type_dict.items(), columns=['documentType', 'documentSupertype'])\n",
    "\n",
    "df_all = pd.merge(df_all, df_docSuper, how='left')\n",
    "        \n",
    "# reoder and rename columns \n",
    "df_all = df_all[['page', 'documentType', 'documentSupertype', 'sessionHitsAll', 'entranceHit', 'exitHit', 'entranceAndExitHit']]\n",
    "df_all = df_all.rename(columns={'documentType': 'document type', 'documentSupertype': 'document supertype', 'sessionHitsAll': 'number of sessions that visit this page', 'entranceHit': 'number of sessions where this page is an entrance hit', 'exitHit': 'number of sessions where this page is an exit hit', 'entranceAndExitHit': 'number of sessions where this page is both an entrance and exit hit'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add ranking to final csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest_seed0_1\n",
    "df_shortest_seed_0_1 = pd.DataFrame(shortest_seed0_1.items(), columns=['page', 'distance from `/browse/working/finding-job` (e.g. the higher the value is, the further away the page is from /browse/working/finding-job)'])\n",
    "df_all = pd.merge(df_all, df_shortest_seed_0_1, how='left')\n",
    "\n",
    "# shortest_seed0_2\n",
    "df_shortest_seed_0_2 = pd.DataFrame(shortest_seed0_2.items(), columns=['page', 'distance from `/topic/further-education-skills/apprenticeships (e.g. the higher the value is, the further away the page is from /topic/further-education-skills/apprenticeships)'])\n",
    "df_all = pd.merge(df_all, df_shortest_seed_0_2, how='left')\n",
    "\n",
    "# where NaN, add 'no path' (NaN = no path is present between the source and target nodes)\n",
    "df_all.fillna('no path', inplace = True)\n",
    "\n",
    "# g_degree\n",
    "df_g_degree = pd.DataFrame(g_degree.items(), columns=['page', 'the number of pages a user moves to/from between this page and another page in the list (e.g. the higher the value, the more users have moved to/from this page and another page in the list)'])\n",
    "df_all = pd.merge(df_all, df_g_degree, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order final csv by rank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.set_index('page')\n",
    "df_all = df_all.reindex(index=ranked_items['page']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('../../data/processed/pages_ranked_with_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6313e0c8b41eb13583c36725f8a1f1fa69ba497bb7e6eaf409642bf47446516"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
