{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the economic recovery whole user journey subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach does not rely on the existing knowledge graph. It uses a functional graph based on page hit session data, to find a list of pages related to the economic recovery whole user journey (WUJ).\n",
    "\n",
    "ASSUMPTIONS: \n",
    "- A set of pre-defined pages have been removed from the economic recovery WUJ subgraph\n",
    "- Any pages with less than 20 session hits are removed from the economic recovery WUJ subgraph\n",
    "- Any pages with a shortest page path length equal to or greater than 3 from both `seed0` pages are removed from the economic recovery WUJ subgraph\n",
    "- Any pages where accumulated edge weight is equal to or lower than 20 are removed from the economic recovery WUJ subgraph\n",
    "- Any browse pages not related to the economic recovery WUJ are manually removed from the economic recovery WUJ subgraph\n",
    "\n",
    "OUTPUT: \n",
    "- A csv containing a list of pages related to the economic recovery WUJ. Sorted in ascending order in relation to a composite metric made up of: shortest page path from both `seed0` pages, closeness centrality, and degree centrality. \n",
    "\n",
    "REQUIREMENTS: \n",
    "- Run `step_one_identify_seed_pages.ipynb` to define `seed0` and `seed1` pages\n",
    "- Run `step_two_extract_page_hits.sql` to extract page hits for sessions that visit at least one `seed0` or `seed1` page\n",
    "- Run `step_three_extract_nodes_and_edges.sql` to extract nodes and edges \n",
    "- Run `step_four_create_networkx_graph.ipynb` to create NetworkX graph of the economic recovery whole user journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import networkx as nx\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import operator\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import gspread \n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for coercing knowledge graph into NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSubgraph(q, parameters=None):\n",
    "\n",
    "    '''\n",
    "    Given a Cypher query q, this function queries the knowledge graph,\n",
    "    returns the nodes and edges from this query, and uses them to construct\n",
    "    a networkx graph.\n",
    "\n",
    "    E.g. getSubgraph(r'MATCH (u:Cid)-[r:HYPERLINKS_TO]->(v:Cid) RETURN *')\n",
    "         returns the structural graph.\n",
    "\n",
    "    Optionally, can add in parameters (dictionary), allowing Python variables\n",
    "    to be integrated into the Cypher query q.\n",
    "\n",
    "    E.g.\n",
    "        parameters = {}\n",
    "        parameters['pages'] = ['a','list','of','stuff']\n",
    "        q7 = f\"\"\"\n",
    "        MATCH (u:Cid)-[r]-(v:Cid)\n",
    "        WHERE u.name IN $pages AND v.name in $pages\n",
    "        RETURN *\n",
    "        \"\"\"\n",
    "\n",
    "        g7 = getSubgraph(q7, parameters)\n",
    "    '''\n",
    "\n",
    "    # get credentials\n",
    "    # add to .secrets: export KG_PWD=\"<PASSWORD>\"\n",
    "    KG_PWD = os.getenv(\"KG_PWD\")\n",
    "\n",
    "    # create connection to knowledge graph\n",
    "    driver = GraphDatabase.driver(\n",
    "        \"bolt+s://knowledge-graph.integration.govuk.digital:7687\",\n",
    "        auth=(\"neo4j\", KG_PWD),\n",
    "    )\n",
    "\n",
    "    # run query on knowledge graph\n",
    "    results = driver.session().run(q, parameters)\n",
    "\n",
    "    # create networkx graph object\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # add nodes into networkx graph object\n",
    "    nodes = list(results.graph()._nodes.values())\n",
    "    print(\"Adding nodes\\n\")\n",
    "    for node in tqdm(nodes):\n",
    "        G.add_node(node.id, labels=node._labels, properties=node._properties)\n",
    "\n",
    "    # add edges into networkx graph object\n",
    "    rels = list(results.graph()._relationships.values())\n",
    "    print(\"Adding edges\\n\")\n",
    "    for rel in tqdm(rels):\n",
    "        G.add_edge(\n",
    "            rel.start_node.id,\n",
    "            rel.end_node.id,\n",
    "            key=rel.id,\n",
    "            type=rel.type,\n",
    "            properties=rel._properties,\n",
    "        )\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def showGraph(g):\n",
    "    \"\"\"\n",
    "    Given a networkx graph g, this function visualises the graph.\n",
    "    Do not use for a large g.\n",
    "    \"\"\"\n",
    "    print(nx.info(g))\n",
    "    nx.draw(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoOfTruePages(g):\n",
    "    \"\"\"\n",
    "    Calculate a proxy recall metric for the list of pages identified in a\n",
    "    subgraph (when compared to the ground truth for the economic recovery pages). \n",
    "    The output is the number of pages in the subgraph list that are present in \n",
    "    the ground truth list.  \n",
    "    \"\"\"\n",
    "    \n",
    "    # convert nodeIds to page path slug for the subgraph list\n",
    "    subgraph_list = [node[0] for node in g.nodes(data=True)]\n",
    "\n",
    "    # set up the ground truth list\n",
    "    true_list = list(economic_pages)\n",
    "\n",
    "    # how many pages are in the subgraph list that are also in the ground truth list\n",
    "    return (len(true_list)) - (len([node for node in true_list if node not in subgraph_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPagesNotInSubGraph(g):\n",
    "    \"\"\"\n",
    "    Return a set of pages which are in the economic recovery pre-defined list\n",
    "    (41 in total), but are not in the filtered subgraph list. \n",
    "    \"\"\"\n",
    "    \n",
    "    # list of pages manually defined in the economic recovery whole user journey \n",
    "    true_list = list(economic_pages)\n",
    "    \n",
    "    # list of pages in the filtered subgraph list\n",
    "    subgraph_list = [node[0] for node in g.nodes(data=True)]\n",
    "    \n",
    "    # the list of pages in the manually defined list not in the subgraph list\n",
    "    return [node for node in true_list if node not in subgraph_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-defined economic recovery subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pages (nodes) in the manually defined list for economic recovery (41 pages in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_APPLICATION_CREDENTIALS = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')\n",
    "\n",
    "# Connect to service account\n",
    "scope = ['https://spreadsheets.google.com/feeds'] \n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(GOOGLE_APPLICATION_CREDENTIALS, scope) \n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "# Import the data from google sheets\n",
    "spreadsheet_key = '1lLsgQRsl4bXmbyiwrbMNQOR_Zu-FkbqYRUmOOhTYc70' \n",
    "book = gc.open_by_key(spreadsheet_key) \n",
    "worksheet = book.worksheet('Top pages') \n",
    "table = worksheet.get_all_values()\n",
    "\n",
    "# Convert table data into a dataframe then set \n",
    "df = pd.DataFrame(table[1:], columns=table[0])\n",
    "economic_pages=set(list(filter(None, df['pagePathv2'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the economic recovery functional network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 11677 nodes and 88111 edges\n",
    "- The page `/guidance/international-trade-products-and-schemes` which is in the manually defined graph, is **not** in this subgraph. This seems to be an old page (updated 2013) related to exporting agricultural products to the EU\n",
    "- The distribution of session hit data demonstrates that only 25% of the page paths have session hits over 8. The maximum number of session hit for a page path is 71667: \n",
    "    - Min: 1\n",
    "    - Max: 71667\n",
    "    - 25th percentile: 1\n",
    "    - 50th percentile: 2\n",
    "    - 75th percentile: 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the NetworkX graph object\n",
    "g = nx.read_gpickle(\"../../data/processed/functional_session_hit_directed_graph_er.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes, number of edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the 'true' economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "true_list = list(economic_pages)\n",
    "subgraph_list = [node[0] for node in g.nodes(data=True)]\n",
    "[node for node in true_list if node not in subgraph_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of nodes\n",
    "nodes_list=list(g.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of session hit data \n",
    "x = [y['sessionHits'] for x, y in nodes_list]\n",
    "\n",
    "plt.hist(x); # , bins = 10\n",
    "plt.xlabel('Session hits')\n",
    "plt.ylabel('Count')\n",
    "plt.title('No. of page paths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot to identify outliers\n",
    "plt.boxplot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descriptives, and remove outliers in boxplot\n",
    "plt.boxplot(x, showfliers=False)\n",
    "pd.DataFrame(x).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at distribution of session hit data up to 100\n",
    "x = [y['sessionHits'] for x, y in nodes_list]\n",
    "x=sorted(x, reverse=True)\n",
    "x=x[2:]\n",
    "plt.hist(x,bins=10,range=(0,100))\n",
    "plt.xlabel('Session hits')\n",
    "plt.ylabel('Count')\n",
    "plt.title('No. of session hits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the precision of the subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recall of the subgraph (11677 nodes) is acceptable. However, we need to improve the precision (i.e. reduce the number of nodes) so that a human can manually trawl through the list of pages, and not be overwhelmed. We assume the pages removed are irrelevant.\n",
    "\n",
    "- Remove **irrelevant pages**. There are a number of pages that have high session hit data and are not relevant to the WUJ. These should be predefined and removed. While some would be relevant to remove in all graphs (e.g. `/search/all`), other pages are more unique, but patterns emerge. For example, pages related to `pensions` and `criminal convinctions`. \n",
    "\n",
    "- Remove **session hits equal to or lower than 20**.  \n",
    "\n",
    "- Remove pages where **shortest page path length from is equal to or greater than 3, from both seed0 pages**\n",
    "\n",
    "- Remove pages where the **accumulated edge weight for one page is equal to or greater than 20**\n",
    "\n",
    "This leaves us with a subgraph of: \n",
    "- 1384 nodes\n",
    "- 37/41 nodes in the subgraph, which are also in the manually defined list of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrelevant pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order pages in descending order related to session hits\n",
    "sorted(g.nodes(data=True), key=lambda x: x[1]['sessionHits'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nodes \n",
    "g.remove_nodes_from(['/prove-right-to-work', '/', '/request-copy-criminal-record', '/browse/working/state-pension', \n",
    "                     '/search/all','/search', '/brexit', '/coronavirus', '/report-covid19-result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pages with session hits equal or lower than 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide a cut-off to remove pages with sessions hits lower than X. 20 seems to contain a good number of pre-defined pages (38/41), and reduces the number of nodes in the graph drastically (11677 > 1703).  \n",
    "- Equal or lower than 10: 2450 nodes, 39/41, [`/find-driving-instructor-training`, `/guidance/international-trade-products-and-schemes`]\n",
    "- Equal or lower than 20: 1703 nodes, 38/41, [`/guidance/recovery-loan-scheme`, `/find-driving-instructor-training`, `/guidance/international-trade-products-and-schemes`]\n",
    "- Equal or lower than 30: 1355 nodes, 35/41, [`/guidance/recovery-loan-scheme`, `/find-driving-instructor-training`,\n",
    " `/education`, `/guidance/international-trade-products-and-schemes`, `/agricultural-skills-and-training`, `/government/collections/financial-support-for-businesses-during-coronavirus-covid-19`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table for session hits\n",
    "x = sorted([y['sessionHits'] for x, y in nodes_list], reverse=True)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal to or lower than 10 sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages with less than 10 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHits'] <= 10]\n",
    "\n",
    "# remove nodes with less than 10 session hits \n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# how many nodes and edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the start a business pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal to or lower than 20 sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages with less than 20 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHits'] <= 20]\n",
    "\n",
    "# remove nodes with less than 20 session hits \n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# how many nodes and edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the economic recovery pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equal to or lower than 30 sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pages with less than 30 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHits'] <= 30]\n",
    "\n",
    "# remove nodes with less than 30 session hits \n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# how many nodes and edges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the start a business pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pages with max shortest page paths from `seed0` pages >= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the shortest page paths from seed0 pages to all other pages. \n",
    "\n",
    "Frequency distribution of shortest page path lengths: \n",
    "- `seed0.1`; /browse/working/finding-job; {3: 72, 2: 1484, 1: 146, 0: 1}\n",
    "- `seed0.2`: /topic/further-education-skills/apprenticeships; {3: 173, 2: 1419, 1: 110, 0: 1}\n",
    "\n",
    "\n",
    "If the shortest page path is equal to or greater than 3, remove from the nodes from the subgraph:\n",
    "- 1688 nodes\n",
    "- 38/41 \n",
    "- Pages in the manually defined list not in the subgraph: ['/guidance/recovery-loan-scheme', '/find-driving-instructor-training', '/guidance/international-trade-products-and-schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort nodes by closeness to seed 0 pages\n",
    "shortest_paths_seed0_1 = nx.shortest_path_length(g, source='/browse/working/finding-job')\n",
    "shortest_paths_seed0_2 = nx.shortest_path_length(g, source='/topic/further-education-skills/apprenticeships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort nodes in descending order\n",
    "finding_job={k: v for k, v in sorted(shortest_paths_seed0_1.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort nodes in descending order\n",
    "apprentice={k: v for k, v in sorted(shortest_paths_seed0_2.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table for `seed0.1`\n",
    "x = sorted([y for x, y in finding_job.items()], reverse=True)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table for `seed0.2`\n",
    "x = sorted([y for x, y in apprentice.items()], reverse=True)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract list of pages that have a shortest page path length of >= 3\n",
    "finding_job_filtered=[node for node, length in finding_job.items() if length >= 3]\n",
    "apprentice_filtered=[node for node, length in apprentice.items() if length >= 3]\n",
    "\n",
    "# extract list of page where they have a shortest page path length on >= 3 from both `seed0` pages\n",
    "remove=[node for node in finding_job_filtered if node not in apprentice_filtered]\n",
    "\n",
    "# remove list of pages from g\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the economic recovery pages in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration: remove pages with the shortest page paths summed (`seed0.1` + `seed0.2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could also look at the sum of the shortest page paths. However, the caveat here is that if the shortest page path length is `1` from `seed0.1` to `page A`, but `6` from `seed0.2` to `page A`, then the sum would be `7`. However, `page A` may be very relevant to the WUJ, as defined by it's shortest page path length from `seed0.1`.  \n",
    "\n",
    "Therefore, this method has not been chosen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum all pages shortest page paths of both `seed0` nodes to get an overall shortest page path proxy metric\n",
    "combined_paths = []\n",
    "\n",
    "for page, closeness in finding_job.items():\n",
    "    for page1, closeness1 in apprentice.items():\n",
    "        if page == page1: \n",
    "            combined_score = closeness + closeness1\n",
    "            combined_paths.append(page)\n",
    "            combined_paths.append(combined_score)\n",
    "            \n",
    "combined_paths=dict(zip(combined_paths[::2], combined_paths[1::2]))\n",
    "combined_paths={k: v for k, v in sorted(combined_paths.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nodes above or equal to 6 paths away  \n",
    "remove=[node for node, length in combined_paths.items() if length >= 6]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove pages where edge weight is <= 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge weight increases by 1 each time a user session visits page A to page B. Therefore, if edge weight is small, we assume the two pages are likely to not be associated with one another. This is because we assume that user's will visit similar pages in the same WUJ during the same session. \n",
    "\n",
    "Edge weight is equal to or less than 10:\n",
    "- 1615 nodes\n",
    "- 38/41 \n",
    "- Pages not in the subgraph list: ['/guidance/recovery-loan-scheme', \n",
    "'/find-driving-instructor-training', \n",
    "'/guidance/international-trade-products-and-schemes']\n",
    "\n",
    "Edge weight is equal to or less than 20:\n",
    "- 1384 nodes\n",
    "- 37/41 \n",
    "- Pages not in the subgraph list: ['/guidance/recovery-loan-scheme',\n",
    " '/find-driving-instructor-training',\n",
    " '/guidance/international-trade-products-and-schemes',\n",
    " '/agricultural-skills-and-training']\n",
    "\n",
    "Edge weight is equal to or less than 30:\n",
    "- 1151 nodes\n",
    "- 34/41 \n",
    "- Pages not in the subgraph list: ['/guidance/recovery-loan-scheme',\n",
    " '/find-driving-instructor-training',\n",
    " '/education',\n",
    " '/guidance/plan-for-jobs-skills-and-employment-programmes-information-for-employers',\n",
    " '/guidance/international-trade-products-and-schemes',\n",
    " '/agricultural-skills-and-training',\n",
    " '/government/collections/financial-support-for-businesses-during-coronavirus-covid-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight (user movement) for each pair of nodes in the graph  \n",
    "user_movement_weights = []\n",
    "for node1, node2, edgeWeight in g.edges(data=True):\n",
    "    case = {'node1': node1, 'node2': node2, 'edgeWeight':edgeWeight['edgeWeight']}\n",
    "    user_movement_weights.append(case)\n",
    "\n",
    "#sorted(user_movement_weights, key=itemgetter('edgeWeight'), reverse=True)\n",
    "\n",
    "# sum the weight for each node \n",
    "user_movements_sum = defaultdict(float)\n",
    "\n",
    "for info in user_movement_weights:\n",
    "    user_movements_sum[info['node1']] += info['edgeWeight']\n",
    "\n",
    "user_movements_sum = [{'node1': node1, 'edgeWeight': user_movements_sum[node1]} \n",
    "                     for node1 in user_movements_sum]\n",
    "\n",
    "#sorted(user_movements_sum, key=lambda x: x['edgeWeight'], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency table for user_movements_sum >> TO DO AND SORT OUT\n",
    "x = sorted([y['sessionHits'] for x, y in nodes_list], reverse=True)\n",
    "Counter(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pages where edge weight is less than or equal to 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything with edge weight less than 10 \n",
    "remove=[node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 10]\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pages where edge weight is less than or equal to 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything with edge weight less than 20 \n",
    "remove=[node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 20]\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove pages where edge weight is less than or equal to 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove anything with edge weight less than 30 \n",
    "remove=[node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 30]\n",
    "g.remove_nodes_from(remove)\n",
    "\n",
    "# number of nodes and esges\n",
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the subgraph list of pages that are related to the economic recovery whole user journey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above exploration, run this code to get a final subgraph related to the economic recovery whole user journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the NetworkX graph object\n",
    "g = nx.read_gpickle(\"../../data/processed/functional_session_hit_directed_graph_er.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant nodes \n",
    "g.remove_nodes_from(['/prove-right-to-work', '/', '/request-copy-criminal-record', '/browse/working/state-pension', \n",
    "                     '/search/all','/search', '/brexit', '/coronavirus', '/report-covid19-result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pages with equal or less than 20 session hits \n",
    "remove = [node for node, session in g.nodes(data=True) if session['sessionHits'] <= 20]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pages where they have a shortest page path length on >= 3 from both `seed0` pages\n",
    "shortest_paths_seed0_1 = nx.shortest_path_length(g, source='/browse/working/finding-job')\n",
    "shortest_paths_seed0_2 = nx.shortest_path_length(g, source='/topic/further-education-skills/apprenticeships')\n",
    "\n",
    "finding_job={k: v for k, v in sorted(shortest_paths_seed0_1.items(), key=lambda item: item[1], reverse=True)}\n",
    "apprentice={k: v for k, v in sorted(shortest_paths_seed0_2.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "finding_job_filtered=[node for node, length in finding_job.items() if length >= 3]\n",
    "apprentice_filtered=[node for node, length in apprentice.items() if length >= 3]\n",
    "\n",
    "remove=[node for node in finding_job_filtered if node not in apprentice_filtered]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pages where accumulated edge weight is equal to or lower than 20\n",
    "user_movement_weights = []\n",
    "for node1, node2, edgeWeight in g.edges(data=True):\n",
    "    case = {'node1': node1, 'node2': node2, 'edgeWeight':edgeWeight['edgeWeight']}\n",
    "    user_movement_weights.append(case)\n",
    "\n",
    "user_movements_sum = defaultdict(float)\n",
    "\n",
    "for info in user_movement_weights:\n",
    "    user_movements_sum[info['node1']] += info['edgeWeight']\n",
    "\n",
    "user_movements_sum = [{'node1': node1, 'edgeWeight': user_movements_sum[node1]} \n",
    "                     for node1 in user_movements_sum]\n",
    "\n",
    "remove=[node['node1'] for node in user_movements_sum if node['edgeWeight'] <= 20]\n",
    "g.remove_nodes_from(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of economic recovery pages are in the functional graph?\n",
    "getNoOfTruePages(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which pages are in the economic recovery list but not in the subgraph list? \n",
    "getPagesNotInSubGraph(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final subgraph list related to the economic recovery whole user journey\n",
    "economic_recovery_subgraph = list(g.nodes(data=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking the subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a subgraph of 1384 nodes. How do we rank these pages so that the most 'relevant' pages to a WUJ are at the top, while the most 'irrelevant' pages to a WUJ are at the bottom? \n",
    "\n",
    "Explored:\n",
    "- session hits \n",
    "- shortest page path lengths \n",
    "- sum of the shortest page path lengths\n",
    "- between centrality\n",
    "- degree centrality \n",
    "- closeness centrality \n",
    "\n",
    "An average (non-weighted) rank for each page path was created using:\n",
    "- shortest page path length from seed0.1: `shortest_seed0_1`\n",
    "- shortest page path length from seed0.2: `shortest_seed0_2`\n",
    "- closeness centrality: `g_closeness`\n",
    "- degree centrality: `g_degree`\n",
    "\n",
    "These four metrics were chosen as the assumption is that nodes close to seed0 nodes, are more likely to be associated with the same topic, and therefore the same WUJ. In addition, nodes which are more highly connected (i.e. more outgoing/incoming relationships) should be included in the WUJ, as it is expected the majority of the pages in the subgraph are mostly relevant.   \n",
    "\n",
    "The others metrics were not chosen, because: \n",
    "- session hits: pages with low session hits may be relevant to a WUJ\n",
    "- sum of the shortest page path lengths: pages may be closer to one seed0 node, but far away from the other seed0 node\n",
    "- between centrality: pages relevant to a WUJ may not neccessarily have many paths that pass through. It is likely that more 'popular' pages will (i.e. ones with higher session hits)\n",
    "\n",
    "\n",
    "Caveat: browse pages are likely to be ranked highly, as these are common pages which may be highly connected to other nodes. Therefore, irrelevant browse pages are removed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by session hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = list(sorted(economic_recovery_subgraph, key=lambda x: x[1]['sessionHits'], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by shortest page path lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the paths of the shortest page path\n",
    "shortest_paths_seed0 = nx.shortest_path_length(g, source='/browse/working/finding-job')\n",
    "shortest_paths_seed1 = nx.shortest_path_length(g, source='/topic/further-education-skills/apprenticeships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeed0.1: shortest_paths_seed0.1: '/browse/working/finding-job'\n",
    "shortest_seed0_1 = {k: v for k, v in sorted(shortest_paths_seed0.items(), key=lambda item: item[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed0.2: shortest_paths_seed0.2: '/topic/further-education-skills/apprenticeships'\n",
    "shortest_seed0_2 = {k: v for k, v in sorted(shortest_paths_seed1.items(), key=lambda item: item[1], reverse=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by the sum of the shortest page path lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paths = []\n",
    "\n",
    "for page, closeness in finding_job.items():\n",
    "    for page1, closeness1 in apprentice.items():\n",
    "        if page == page1: \n",
    "            combined_score = closeness + closeness1\n",
    "            combined_paths.append(page)\n",
    "            combined_paths.append(combined_score)\n",
    "            \n",
    "combined_paths=dict(zip(combined_paths[::2], combined_paths[1::2]))\n",
    "combined_paths={k: v for k, v in sorted(combined_paths.items(), key=lambda item: item[1], reverse=True)}\n",
    "combined_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order by centrality metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# between centrality: the number of shortest paths that pass through the node - a 'bridge' between nodes\n",
    "# higher values indicate higher centrality\n",
    "g_di = nx.DiGraph(g)\n",
    "g_di_between = nx.betweenness_centrality(g_di)\n",
    "g_between = dict(sorted(g_di_between.items(), key=operator.itemgetter(1),reverse=True))\n",
    "g_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree centrality: counts the number of incoming and outgoing relationships from a node - 'most connected'\n",
    "# the higher the degree, the more central the node is\n",
    "g_degree = nx.degree_centrality(g)\n",
    "g_degree = dict(sorted(g_degree.items(), key=operator.itemgetter(1),reverse=True))\n",
    "g_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closeness centrality: average length of the shortest path between the node and all other nodes in the graph\n",
    "# higher values of closeness indicate higher centrality\n",
    "g_closeness = nx.closeness_centrality(g)\n",
    "g_closeness = dict(sorted(g_closeness.items(), key=operator.itemgetter(1),reverse=True))\n",
    "g_closeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall rank from multiple ranked items (above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists in order\n",
    "sessions_list=[(index, element[0]) for index, element in enumerate(sessions)]\n",
    "shortest_seed0_1_list=[(index, element) for index, element in enumerate(shortest_seed0_1)]\n",
    "shortest_seed0_2_list=[(index, element) for index, element in enumerate(shortest_seed0_2)]\n",
    "combined_paths_list=[(index, element) for index, element in enumerate(combined_paths)]\n",
    "g_between_list=[(index, element) for index, element in enumerate(g_between)]\n",
    "g_degree_list=[(index, element) for index, element in enumerate(g_degree)]\n",
    "g_closeness_list=[(index, element) for index, element in enumerate(g_closeness)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take average of rank for each pagePath (non-weighted)\n",
    "ranked_items = pd.DataFrame.from_records(shortest_seed0_1_list+shortest_seed0_2_list+g_closeness_list+g_degree_list).groupby(1).mean().round().reset_index()\n",
    "ranked_items = ranked_items.rename(columns={1: 'pagePath', 0: 'rank'})\n",
    "ranked_items = ranked_items.sort_values(by=['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove certain browse pages. browse pages seem to score highly,\n",
    "# regardless of whether they are relevant to the WUJ or not\n",
    "browse_pages_remove = ('/browse/employing-people', '/browse/births-deaths-marriages', \n",
    "                       '/browse/citizenship', '/browse/driving', '/browse/education', \n",
    "                       '/browse/business', '/browse/childcare-parenting', '/browse/justice', \n",
    "                       '/browse/abroad', '/browse/tax', '/browse/visas-immigration', \n",
    "                       '/browse/disabilities', '/browse/environment-countryside', \n",
    "                       '/browse/housing-local-services', '/browse/employing-people', \n",
    "                       '/browse/births-deaths-marriages', '/browse/citizenship', \n",
    "                       '/browse/driving', '/browse/business', '/browse/abroad', \n",
    "                       '/browse/environment-countryside', '/browse/housing-local-services',\n",
    "                       '/browse/benefits')\n",
    "\n",
    "ranked_items = ranked_items[~ranked_items.pagePath.str.startswith(browse_pages_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download as csv file\n",
    "ranked_items.to_csv('../../data/processed/ranked_items.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include other information in final csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract session hits\n",
    "df = pd.DataFrame(sessions, columns=['pagePath', 'sessionHits'])\n",
    "df['sessionHits'] = df['sessionHits'].astype('string')\n",
    "\n",
    "session_hits=df['sessionHits'].str.replace(r'{\\'sessionHits\\': ', '')\n",
    "session_hits=list(session_hits.str.replace(r'}', ''))\n",
    "pages=list(df['pagePath'])\n",
    "\n",
    "pages_session_hits={pages[i]: session_hits[i] for i in range(len(pages))}\n",
    "\n",
    "# dict of ranked items \n",
    "ranked_items_dict=ranked_items.to_dict()[(list(a.keys())[0])]\n",
    "\n",
    "# find session hit for each page path in ranked_items_dict\n",
    "ranked_items_with_sessions = {}\n",
    "for index, page in ranked_items_dict.items():\n",
    "    for page1, hit in pages_session_hits.items():\n",
    "        if page == page1:\n",
    "            ranked_items_with_sessions[page]=hit\n",
    "\n",
    "# rename columns and save ranked_items_with_sessions      \n",
    "ranked_items_df=df.from_dict(ranked_items_with_sessions, orient='index').reset_index()\n",
    "mapping = {ranked_items_df.columns[0]:'page', ranked_items_df.columns[1]: 'session hits'}\n",
    "ranked_items_df=ranked_items_df.rename(columns=mapping)\n",
    "ranked_items_df.to_csv('../../data/processed/ranked_items_with_sessions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6313e0c8b41eb13583c36725f8a1f1fa69ba497bb7e6eaf409642bf47446516"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
