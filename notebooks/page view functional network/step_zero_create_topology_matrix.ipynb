{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create topology sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach does not rely on the existing knowledge graph. A functional graph based on page hit session data is created, before further filtering of the graph, to find a list of pages related to the economic recovery whole user journey (WUJ). \n",
    "\n",
    "The first step is to identify seed0 and seed1 pages. Seed0 pages must be pre-defined and manually entered as `SEED0_PAGES`. Then, a topology sparse matrix is created which represents how each seed0 page is connected to another page via hyperlink. Adpated from: https://github.com/alphagov/govuk-intent-detector/blob/main/notebooks/generate_topology_matrix.ipynb\n",
    "\n",
    "ASSUMPTIONS: \n",
    "- We are removing links to cross-domain services / external domain\n",
    "- We attach anchor to the main url (/business#content)\n",
    "- We are keeping the step-by-step ID, and search parameters\n",
    "- We are only keeping self-loops where a page contains an explicit link to itself \n",
    "- We are not accounting for the fact that any page can be reloaded -> if we want this, add 1's to whole diagonal\n",
    "- If we do not want any of these two options (point above) need remove add 0's to whole diagonal AND remove self-loop from user journey data \n",
    "- `SEED0_PAGES` are defined as `/topic/further-education-skills` and `/browse/working/finding-job`. These were chosen as they are topic and browse pages, which therefore link to many similar pages. This analysis assumes these pages are important pages in the economic recovery whole user journey. `SEED1_PAGES` are reliant on `SEED0_PAGES`, therefore this analysis is dependent on what the `SEED0_PAGES` are.\n",
    "- Footer pages are manually defined and removed from the final list of `SEED1_PAGES`. This is because they occur on every page, regardless of the Whole User Journey. \n",
    "\n",
    "OUTPUT: \n",
    "- Saves `{date}_govuk_topology_matrix.pickle` in `../data/interim`\n",
    "- A list of `SEED1_PAGES` are downloaded locally as `seed1_economic_recovery.csv` \n",
    "\n",
    "REQUIREMENTS:\n",
    "- No scripts need to be run before this script\n",
    "- Imports module `make_topology_matrix.py` located in `../src/make_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.element import Doctype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.make_data.make_topology_matrix import (clean_url, combine_anchor2url, process_page_links, \n",
    "                                                create_topology_matrix_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the raw data folder where HTML pages will be stored\n",
    "DIR_DATA_RAW = os.getenv(\"DIR_DATA_RAW\")\n",
    "\n",
    "# Create a new folder with the current timestamp - content may change, so it's good to keep a local record\n",
    "HTML_DATETIME = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "DIR_HTML = Path(DIR_DATA_RAW, \"html\", HTML_DATETIME)\n",
    "DIR_HTML.mkdir(parents=True)\n",
    "\n",
    "# Output folder\n",
    "DIR_DATA_INTERIM = Path(os.getenv(\"DIR_DATA_INTERIM\"))\n",
    "\n",
    "# Define the output file path\n",
    "OUTPUT_FILE = DIR_DATA_INTERIM.joinpath(f\"{HTML_DATETIME}_govuk_topology_matrix.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated user journeys urls\n",
    "DIR_DATA_EXTERNAL = os.environ.get(\"DIR_DATA_EXTERNAL\")\n",
    "SEED0_PAGES = ['/topic/further-education-skills', '/browse/working/finding-job']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get GOVUK html pages for seed0 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialise an empty dictionary to store the HTML pages\n",
    "html_pages = {}\n",
    "\n",
    "# Iterate over the requested GOV.UK pages\n",
    "for page in tqdm(SEED0_PAGES):\n",
    "    \n",
    "    # Raise an error if the page doesn't start with a \"/\" - prevents the request from hanging\n",
    "    if not page.startswith(\"/\"):\n",
    "        raise ValueError(f\"Pages must start with '/': {page}\")\n",
    "    \n",
    "    # Download the HTML page, store it in `html_pages` \n",
    "    with request.urlopen(f\"https://www.gov.uk{page}\") as hp:\n",
    "         html_page = hp.read().decode(\"utf8\")\n",
    "    \n",
    "    # Check if there is an anchor heading in the page URL; if there is one, only get all the HTML **after** the\n",
    "    # anchor\n",
    "    anchor_heading = re.match(r\".*#(?P<anchor>[^/]+)$\", page, flags=re.DOTALL)\n",
    "    if anchor_heading:\n",
    "        heading_string = str(BeautifulSoup(html_page).find(id=anchor_heading.group(\"anchor\")))\n",
    "        html_page = heading_string + html_page.split(heading_string)[1]\n",
    "    \n",
    "    # Write `html_page` out, replacing \"/\" with \"__\", as \"/\" is not a valid file name, and also save it in a \n",
    "    # dictionary for further analysis\n",
    "    with open(Path(DIR_HTML, f\"{page.replace('/', '__')}.html\"), \"w\") as f:\n",
    "        _ = f.write(html_page)\n",
    "    html_pages[page] = html_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest the html pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 17.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialise an empty dictionary, and extract out the embedding hyperlinks in all the HTML pages\n",
    "# The HTML <a> tag defines a hyperlink. It has the following syntax: <a href=\"url\">link text</a>\n",
    "page_links = {}\n",
    "\n",
    "# Iterate over the HTML files\n",
    "for html_page, html_contents in tqdm(html_pages.items()):\n",
    "    \n",
    "    # Extract all embedded hyperlinks and save them in a list\n",
    "    links = BeautifulSoup(html_contents, parse_only=SoupStrainer('a')) \n",
    "    page_links[html_page] = [link.get('href') for link in links if not isinstance(link, Doctype)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate topology matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process hyperlinks embedded in each page\n",
    "page_links_proc = process_page_links(page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate directed topology matrix, as apandas.DataFrame\n",
    "# Note that we keep source urls and destination urls\n",
    "topology_matrix_df = create_topology_matrix_pd(page_links=page_links_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save matrix, and list of seed1 pages (with footer pages removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save matrix\n",
    "topology_matrix_df.to_pickle(path=OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df of seed1 pages\n",
    "df = pd.DataFrame(topology_matrix_df.columns.values.tolist(), columns=[\"seed1_page\"])\n",
    "\n",
    "# Remove footer pages\n",
    "footer_pages = ['/browse/disabilities',\n",
    "                '/browse/housing-local-services',\n",
    "                '/help',\n",
    "                '/browse/tax',\n",
    "                '/browse/childcare-parenting',\n",
    "                '/',\n",
    "                '/browse/employing-people',\n",
    "                '/browse/environment-countryside',\n",
    "                '/government/organisations/government-digital-service',\n",
    "                '/help/terms-conditions',\n",
    "                '/browse/benefits',\n",
    "                '/help/cookies',\n",
    "                '/browse/births-deaths-marriages',\n",
    "                '/browse/abroad',\n",
    "                '/coronavirus',\n",
    "                '/contact',\n",
    "                '/transition',\n",
    "                '/government/how-government-works',\n",
    "                '/browse/education',\n",
    "                '/browse/justice',\n",
    "                '/browse/citizenship',\n",
    "                '/cymraeg',\n",
    "                '/browse/working',\n",
    "                '/browse/business',\n",
    "                '/help/accessibility-statement',\n",
    "                '/world',\n",
    "                '/browse/visas-immigration',\n",
    "                '/browse/driving',\n",
    "                '/help/privacy-notice',\n",
    "                '/government/organisations'\n",
    "               ]\n",
    "    \n",
    "df = df[~df.seed1_page.isin(footer_pages)]\n",
    "\n",
    "# Save df to csv\n",
    "df.to_csv('../../data/interim/seed1_economic_recovery.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
